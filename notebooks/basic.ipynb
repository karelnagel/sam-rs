{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "901c8ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1662bb7c",
   "metadata": {},
   "source": [
    "# Produces masks from prompts using an ONNX model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcc21a0",
   "metadata": {},
   "source": [
    "SAM's prompt encoder and mask decoder are very lightweight, which allows for efficient computation of a mask given user input. This notebook shows an example of how to export and use this lightweight component of the model in ONNX format, allowing it to run on a variety of platforms that support an ONNX runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "86daff77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<a target=\"_blank\" href=\"https://colab.research.google.com/github/facebookresearch/segment-anything/blob/main/notebooks/onnx_model_example.ipynb\">\n",
       "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
       "</a>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\n",
    "\"\"\"\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/facebookresearch/segment-anything/blob/main/notebooks/onnx_model_example.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\"\"\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4a58be",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42396e8d",
   "metadata": {},
   "source": [
    "Note that this notebook requires both the `onnx` and `onnxruntime` optional dependencies, in addition to `opencv-python` and `matplotlib` for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2c712610",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "\n",
    "import onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4bfcdc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_mask(mask):\n",
    "    color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    \n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    print(h, w, mask_image)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927a928b",
   "metadata": {},
   "source": [
    "## Example Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6be6eb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('images/truck.jpg')\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027b177b",
   "metadata": {},
   "source": [
    "## Using an ONNX model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778d4593",
   "metadata": {},
   "source": [
    "Here as an example, we use `onnxruntime` in python on CPU to execute the ONNX model. However, any platform that supports an ONNX runtime could be used in principle. Launch the runtime session below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9689b1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_path = \"sam_onnx_quantized_example.onnx\"\n",
    "ort_session = onnxruntime.InferenceSession(onnx_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7708ead6",
   "metadata": {},
   "source": [
    "To use the ONNX model, the image must first be pre-processed using the SAM image encoder. This is a heavier weight process best performed on GPU. SamPredictor can be used as normal, then `.get_image_embedding()` will retreive the intermediate features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "26e067b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"../sam_vit_h_4b8939.pth\"\n",
    "model_type = \"vit_h\"\n",
    "\n",
    "sam = sam_model_registry[model_type](checkpoint=checkpoint)\n",
    "sam.to()\n",
    "predictor = SamPredictor(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7ad3f0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.set_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8a6f0f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embedding = predictor.get_image_embedding().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5e112f33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 256, 64, 64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6337b654",
   "metadata": {},
   "source": [
    "The ONNX model has a different input signature than `SamPredictor.predict`. The following inputs must all be supplied. Note the special cases for both point and mask inputs. All inputs are `np.float32`.\n",
    "* `image_embeddings`: The image embedding from `predictor.get_image_embedding()`. Has a batch index of length 1.\n",
    "* `point_coords`: Coordinates of sparse input prompts, corresponding to both point inputs and box inputs. Boxes are encoded using two points, one for the top-left corner and one for the bottom-right corner. *Coordinates must already be transformed to long-side 1024.* Has a batch index of length 1.\n",
    "* `point_labels`: Labels for the sparse input prompts. 0 is a negative input point, 1 is a positive input point, 2 is a top-left box corner, 3 is a bottom-right box corner, and -1 is a padding point. *If there is no box input, a single padding point with label -1 and coordinates (0.0, 0.0) should be concatenated.*\n",
    "* `mask_input`: A mask input to the model with shape 1x1x256x256. This must be supplied even if there is no mask input. In this case, it can just be zeros.\n",
    "* `has_mask_input`: An indicator for the mask input. 1 indicates a mask input, 0 indicates no mask input.\n",
    "* `orig_im_size`: The size of the input image in (H,W) format, before any transformation. \n",
    "\n",
    "Additionally, the ONNX model does not threshold the output mask logits. To obtain a binary mask, threshold at `sam.mask_threshold` (equal to 0.0)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5a9f55",
   "metadata": {},
   "source": [
    "### Example point input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1c0deef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_point = np.array([[500, 875]])\n",
    "input_label = np.array([1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7256394c",
   "metadata": {},
   "source": [
    "Add a batch index, concatenate a padding point, and transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4f69903e",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_coord = np.concatenate([input_point, np.array([[0.0, 0.0]])], axis=0)[None, :, :]\n",
    "onnx_label = np.concatenate([input_label, np.array([-1])], axis=0)[None, :].astype(np.float32)\n",
    "\n",
    "onnx_coord = predictor.transform.apply_coords(onnx_coord, image.shape[:2]).astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b188dc53",
   "metadata": {},
   "source": [
    "Create an empty mask input and an indicator for no mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5cb52bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_mask_input = np.zeros((1, 1, 256, 256), dtype=np.float32)\n",
    "onnx_has_mask_input = np.zeros(1, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99c2cc5",
   "metadata": {},
   "source": [
    "Package the inputs to run in the onnx model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b1d7ea11",
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_inputs = {\n",
    "    \"image_embeddings\": image_embedding,\n",
    "    \"point_coords\": onnx_coord,\n",
    "    \"point_labels\": onnx_label,\n",
    "    \"mask_input\": onnx_mask_input,\n",
    "    \"has_mask_input\": onnx_has_mask_input,\n",
    "    \"orig_im_size\": np.array(image.shape[:2], dtype=np.float32)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6409c9",
   "metadata": {},
   "source": [
    "Predict a mask and threshold it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dc4cc082",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[False False False ... False False False]\n",
      "   [False False False ... False False False]\n",
      "   [False False False ... False False False]\n",
      "   ...\n",
      "   [False False False ... False False False]\n",
      "   [False False False ... False False False]\n",
      "   [False False False ... False False False]]]]\n"
     ]
    }
   ],
   "source": [
    "masks, _, low_res_logits = ort_session.run(None, ort_inputs)\n",
    "masks = masks > predictor.model.mask_threshold\n",
    "print(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d778a8fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 1200, 1800)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "badb1175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200 1800 [[[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "show_mask(masks, )\n",
    "# show_points(input_point, input_label, plt.gca())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
