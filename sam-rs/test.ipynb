{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7ad3bf14",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04ec4816",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import sys\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Item:\n",
    "    def __init__(self, key, value, type:str):\n",
    "        self.key = key\n",
    "        self.type = type\n",
    "        if type.startswith(\"Tensor\"):\n",
    "            self.value = {\"size\":value.size(),\"values\":value.flatten().tolist()}\n",
    "        else:\n",
    "            self.value = value\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {self.key: {self.type: self.value}}\n",
    "\n",
    "\n",
    "def to_file(name:str,items:list):\n",
    "    path = \"test-files/\"+name+\".json\"\n",
    "    values = {}\n",
    "    for item in items:\n",
    "        values.update(item.to_dict())\n",
    "    output = {\"values\": values}\n",
    "    \n",
    "    data = json.dumps(output, indent=4)\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, \"w\") as f:\n",
    "        f.write(data)\n",
    "\n",
    "def random_tensor(shape:list,seed:int=0):\n",
    "    n = 1 \n",
    "    for dim in shape:\n",
    "        n*=dim\n",
    "\n",
    "    a = 3\n",
    "    c = 23\n",
    "    m = 2**32\n",
    "    \n",
    "    result = []\n",
    "    x = seed\n",
    "    for _ in range(n):\n",
    "        x = (a * x + c) % m\n",
    "        result.append(x / m)  # Normalize the result to [0, 1]\n",
    "\n",
    "    return torch.tensor(result).view(shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c0efaef2",
   "metadata": {},
   "source": [
    "## Common\n",
    "#### LayerNorm2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f813cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from segment_anything.modeling.common import LayerNorm2d\n",
    "\n",
    "layer_norm = LayerNorm2d(256,0.1)\n",
    "items = [Item(\"weight\", layer_norm.weight, \"TensorFloat\"), Item(\"bias\", layer_norm.bias, \"TensorFloat\"), Item(\"eps\", layer_norm.eps, \"Float\")]\n",
    "to_file(\"layer_norm_2d\",items)\n",
    "\n",
    "# Forward\n",
    "input = random_tensor([2,256,16,16])\n",
    "output = layer_norm(input)\n",
    "items = [Item(\"input\", input, \"TensorFloat\"), Item(\"output\", output, \"TensorFloat\")]\n",
    "to_file(\"layer_norm_2d_forward\",items)\n",
    "del layer_norm, input, output,items"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d6a99864",
   "metadata": {},
   "source": [
    "#### MLPBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6867373",
   "metadata": {},
   "outputs": [],
   "source": [
    "from segment_anything.modeling.common import MLPBlock\n",
    "\n",
    "mlp_block = MLPBlock(256,256,nn.GELU)\n",
    "mlp_block.lin1.weight.data = random_tensor([256,256],1)\n",
    "mlp_block.lin2.weight.data = random_tensor([256,256],2)\n",
    "mlp_block.lin1.bias.data = random_tensor([256],3)\n",
    "mlp_block.lin2.bias.data = random_tensor([256],4)\n",
    "items=[Item(\"lin1\", mlp_block.lin1.weight, \"TensorFloat\"), Item(\"lin2\", mlp_block.lin2.weight, \"TensorFloat\")]\n",
    "to_file(\"mlp_block\",items)\n",
    "\n",
    "# Forward\n",
    "input = random_tensor([256,256],5)\n",
    "output = mlp_block(input)\n",
    "items = [Item(\"input\", input, \"TensorFloat\"), Item(\"output\", output, \"TensorFloat\")]\n",
    "to_file(\"mlp_block_forward\",items)\n",
    "del mlp_block, input, output,items"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cad0ddd8",
   "metadata": {},
   "source": [
    "#### Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39781f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gelu\n",
    "gelu = nn.GELU()\n",
    "input = random_tensor([256,256])\n",
    "output = gelu(input)\n",
    "items = [Item(\"input\", input, \"TensorFloat\"), Item(\"output\", output, \"TensorFloat\")]\n",
    "to_file(\"activation_gelu\",items)\n",
    "\n",
    "# ReLU\n",
    "relu = nn.ReLU()\n",
    "input = random_tensor([256,256])\n",
    "output = relu(input)\n",
    "items = [Item(\"input\", input, \"TensorFloat\"), Item(\"output\", output, \"TensorFloat\")]\n",
    "to_file(\"activation_relu\",items)\n",
    "del input,output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "74fade84",
   "metadata": {},
   "source": [
    "# Image encoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "56f80aac",
   "metadata": {},
   "source": [
    "#### PatchEmbeded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08dd2d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from segment_anything.modeling.image_encoder import PatchEmbed\n",
    "\n",
    "patch_embed = PatchEmbed((16,16),(16,16),(0,0),3,768)\n",
    "patch_embed.proj.weight.data = random_tensor([2,256,16,16],1)\n",
    "patch_embed.proj.bias.data = random_tensor([2],2)\n",
    "items=[Item(\"weight\", patch_embed.proj.weight, \"TensorFloat\"),Item(\"bias\", patch_embed.proj.bias, \"TensorFloat\")]\n",
    "to_file(\"patch_embed\",items)\n",
    "\n",
    "# Forward\n",
    "input = random_tensor([2,256,16,16],3)\n",
    "output = patch_embed(input)\n",
    "items = [Item(\"input\", input, \"TensorFloat\"), Item(\"output\", output, \"TensorFloat\")]\n",
    "to_file(\"patch_embed_forward\",items)\n",
    "del patch_embed, input, output,items"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7f413326",
   "metadata": {},
   "source": [
    "#### Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d09b671",
   "metadata": {},
   "outputs": [],
   "source": [
    "from segment_anything.modeling.image_encoder import  window_partition,window_unpartition\n",
    "\n",
    "# Window partition\n",
    "input = random_tensor([2,256,16,16],1)\n",
    "output,size = window_partition(input,16)\n",
    "items = [Item(\"input\", input, \"TensorFloat\"), Item(\"output\", output, \"TensorFloat\"), Item(\"size\", size, \"Size\")]\n",
    "to_file(\"window_partition\",items)\n",
    "\n",
    "# Window unpartition\n",
    "input = random_tensor([2,256,16,16],2)\n",
    "output = window_unpartition(input,16,(16,16),(14,14))\n",
    "items = [Item(\"input\", input, \"TensorFloat\"), Item(\"output\", output, \"TensorFloat\")]\n",
    "to_file(\"window_unpartition\",items)\n",
    "del input, output, items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ba0e9dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m output \u001b[39m=\u001b[39m block(\u001b[39minput\u001b[39m)\n\u001b[1;32m     23\u001b[0m items \u001b[39m=\u001b[39m [Item(\u001b[39m\"\u001b[39m\u001b[39minput\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39minput\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mTensorFloat\u001b[39m\u001b[39m\"\u001b[39m), Item(\u001b[39m\"\u001b[39m\u001b[39moutput\u001b[39m\u001b[39m\"\u001b[39m, output, \u001b[39m\"\u001b[39m\u001b[39mTensorFloat\u001b[39m\u001b[39m\"\u001b[39m)]\n\u001b[0;32m---> 24\u001b[0m to_file(\u001b[39m\"\u001b[39;49m\u001b[39mblock_forward\u001b[39;49m\u001b[39m\"\u001b[39;49m,items)\n",
      "Cell \u001b[0;32mIn[5], line 24\u001b[0m, in \u001b[0;36mto_file\u001b[0;34m(name, items)\u001b[0m\n\u001b[1;32m     21\u001b[0m     values\u001b[39m.\u001b[39mupdate(item\u001b[39m.\u001b[39mto_dict())\n\u001b[1;32m     22\u001b[0m output \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mvalues\u001b[39m\u001b[39m\"\u001b[39m: values}\n\u001b[0;32m---> 24\u001b[0m data \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39;49mdumps(output, indent\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m)\n\u001b[1;32m     25\u001b[0m os\u001b[39m.\u001b[39mmakedirs(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mdirname(path), exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     26\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(path, \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.9/3.9.16/Frameworks/Python.framework/Versions/3.9/lib/python3.9/json/__init__.py:234\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m JSONEncoder\n\u001b[0;32m--> 234\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m(\n\u001b[1;32m    235\u001b[0m     skipkeys\u001b[39m=\u001b[39;49mskipkeys, ensure_ascii\u001b[39m=\u001b[39;49mensure_ascii,\n\u001b[1;32m    236\u001b[0m     check_circular\u001b[39m=\u001b[39;49mcheck_circular, allow_nan\u001b[39m=\u001b[39;49mallow_nan, indent\u001b[39m=\u001b[39;49mindent,\n\u001b[1;32m    237\u001b[0m     separators\u001b[39m=\u001b[39;49mseparators, default\u001b[39m=\u001b[39;49mdefault, sort_keys\u001b[39m=\u001b[39;49msort_keys,\n\u001b[1;32m    238\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\u001b[39m.\u001b[39;49mencode(obj)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.9/3.9.16/Frameworks/Python.framework/Versions/3.9/lib/python3.9/json/encoder.py:201\u001b[0m, in \u001b[0;36mJSONEncoder.encode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    199\u001b[0m chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39miterencode(o, _one_shot\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    200\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(chunks, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[0;32m--> 201\u001b[0m     chunks \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(chunks)\n\u001b[1;32m    202\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(chunks)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.9/3.9.16/Frameworks/Python.framework/Versions/3.9/lib/python3.9/json/encoder.py:431\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[39myield from\u001b[39;00m _iterencode_list(o, _current_indent_level)\n\u001b[1;32m    430\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(o, \u001b[39mdict\u001b[39m):\n\u001b[0;32m--> 431\u001b[0m     \u001b[39myield from\u001b[39;00m _iterencode_dict(o, _current_indent_level)\n\u001b[1;32m    432\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    433\u001b[0m     \u001b[39mif\u001b[39;00m markers \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.9/3.9.16/Frameworks/Python.framework/Versions/3.9/lib/python3.9/json/encoder.py:405\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    404\u001b[0m             chunks \u001b[39m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[0;32m--> 405\u001b[0m         \u001b[39myield from\u001b[39;00m chunks\n\u001b[1;32m    406\u001b[0m \u001b[39mif\u001b[39;00m newline_indent \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    407\u001b[0m     _current_indent_level \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.9/3.9.16/Frameworks/Python.framework/Versions/3.9/lib/python3.9/json/encoder.py:405\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    404\u001b[0m             chunks \u001b[39m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[0;32m--> 405\u001b[0m         \u001b[39myield from\u001b[39;00m chunks\n\u001b[1;32m    406\u001b[0m \u001b[39mif\u001b[39;00m newline_indent \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    407\u001b[0m     _current_indent_level \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "    \u001b[0;31m[... skipping similar frames: _make_iterencode.<locals>._iterencode_dict at line 405 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.9/3.9.16/Frameworks/Python.framework/Versions/3.9/lib/python3.9/json/encoder.py:405\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    404\u001b[0m             chunks \u001b[39m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[0;32m--> 405\u001b[0m         \u001b[39myield from\u001b[39;00m chunks\n\u001b[1;32m    406\u001b[0m \u001b[39mif\u001b[39;00m newline_indent \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    407\u001b[0m     _current_indent_level \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.9/3.9.16/Frameworks/Python.framework/Versions/3.9/lib/python3.9/json/encoder.py:316\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_list\u001b[0;34m(lst, _current_indent_level)\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[39myield\u001b[39;00m buf \u001b[39m+\u001b[39m _intstr(value)\n\u001b[1;32m    314\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, \u001b[39mfloat\u001b[39m):\n\u001b[1;32m    315\u001b[0m     \u001b[39m# see comment above for int\u001b[39;00m\n\u001b[0;32m--> 316\u001b[0m     \u001b[39myield\u001b[39;00m buf \u001b[39m+\u001b[39m _floatstr(value)\n\u001b[1;32m    317\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m     \u001b[39myield\u001b[39;00m buf\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.9/3.9.16/Frameworks/Python.framework/Versions/3.9/lib/python3.9/json/encoder.py:229\u001b[0m, in \u001b[0;36mJSONEncoder.iterencode.<locals>.floatstr\u001b[0;34m(o, allow_nan, _repr, _inf, _neginf)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfloatstr\u001b[39m(o, allow_nan\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mallow_nan,\n\u001b[1;32m    224\u001b[0m         _repr\u001b[39m=\u001b[39m\u001b[39mfloat\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__repr__\u001b[39m, _inf\u001b[39m=\u001b[39mINFINITY, _neginf\u001b[39m=\u001b[39m\u001b[39m-\u001b[39mINFINITY):\n\u001b[1;32m    225\u001b[0m     \u001b[39m# Check for specials.  Note that this type of test is processor\u001b[39;00m\n\u001b[1;32m    226\u001b[0m     \u001b[39m# and/or platform-specific, so do tests which don't depend on the\u001b[39;00m\n\u001b[1;32m    227\u001b[0m     \u001b[39m# internals.\u001b[39;00m\n\u001b[0;32m--> 229\u001b[0m     \u001b[39mif\u001b[39;00m o \u001b[39m!=\u001b[39;49m o:\n\u001b[1;32m    230\u001b[0m         text \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mNaN\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    231\u001b[0m     \u001b[39melif\u001b[39;00m o \u001b[39m==\u001b[39m _inf:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from segment_anything.modeling.image_encoder import Block\n",
    "\n",
    "#Block\n",
    "block = Block(1280,16,4.0,True,nn.LayerNorm,nn.GELU,True,True,14,(64,64))\n",
    "items=[Item(\"window_size\", block.window_size, \"Int\")]\n",
    "to_file(\"block\",items)\n",
    "\n",
    "#Forward\n",
    "input = random_tensor([1,64,64,1280],1)\n",
    "block.norm1.weight.data = random_tensor([1280],2)\n",
    "block.norm1.bias.data = random_tensor([1280],3)\n",
    "block.norm2.weight.data = random_tensor([1280],4)\n",
    "block.norm2.bias.data = random_tensor([1280],5)\n",
    "block.attn.qkv.weight.data = random_tensor([3840,1280],6)\n",
    "block.attn.qkv.bias.data = random_tensor([3840],7)\n",
    "block.attn.proj.weight.data = random_tensor([1280,1280],8)\n",
    "block.attn.proj.bias.data = random_tensor([1280],9)\n",
    "block.mlp.lin1.weight.data = random_tensor([5120,1280],10)\n",
    "block.mlp.lin1.bias.data = random_tensor([5120],11)\n",
    "block.mlp.lin2.weight.data = random_tensor([1280,5120],12)\n",
    "block.mlp.lin2.bias.data = random_tensor([1280],13)\n",
    "output = block(input)\n",
    "items = [Item(\"input\", input, \"TensorFloat\"), Item(\"output\", output, \"TensorFloat\")]\n",
    "to_file(\"block_forward\",items)\n",
    "del block, input, output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "312867d4",
   "metadata": {},
   "source": [
    "#### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487ac328",
   "metadata": {},
   "outputs": [],
   "source": [
    "from segment_anything.modeling.image_encoder import get_rel_pos,add_decomposed_rel_pos\n",
    "\n",
    "# Get rel pos\n",
    "q_size = 64\n",
    "k_size = 64\n",
    "input = random_tensor([127,80],1)\n",
    "output = get_rel_pos( q_size, k_size, input)\n",
    "items = [Item(\"input\",input,\"TensorFloat\"),Item(\"output\", output, \"TensorFloat\")]\n",
    "to_file(\"get_rel_pos\",items)\n",
    "del input, output\n",
    "\n",
    "\n",
    "# Add decomposed rel pos\n",
    "attn = random_tensor([400,196,196],2)\n",
    "q = random_tensor([400,196,80],3)\n",
    "relo_pos_h = random_tensor([27,80],4)\n",
    "relo_pos_w = random_tensor([27,80],5)\n",
    "q_size = (14,14)\n",
    "k_size = (14,14)\n",
    "rel_pos = add_decomposed_rel_pos(attn,q,relo_pos_h,relo_pos_w,q_size,k_size)\n",
    "items = [Item(\"attn\", attn, \"TensorFloat\"), Item(\"q\", q, \"TensorFloat\"), Item(\"q_size\", q_size, \"Size\"), Item(\"k_size\", k_size, \"Size\"), Item(\"output\", rel_pos, \"TensorFloat\")]\n",
    "to_file(\"add_decomposed_rel_pos\",items)\n",
    "del attn,q,relo_pos_h,relo_pos_w,q_size,k_size,rel_pos,items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57536f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([400, 196, 80]) torch.Size([400, 196, 80]) torch.Size([400, 196, 80])\n",
      "torch.Size([400, 196, 80]) torch.Size([400, 80, 196])\n"
     ]
    }
   ],
   "source": [
    "from segment_anything.modeling.image_encoder import Attention\n",
    "\n",
    "# Attention\n",
    "attention = Attention(1280, 16 ,True ,True ,True, (14, 14))\n",
    "items =[Item(\"num_heads\", attention.num_heads, \"Int\"), Item(\"scale\", attention.scale, \"Float\"),  Item(\"use_rel_pos\", attention.use_rel_pos, \"Bool\")]\n",
    "to_file(\"attention\",items)\n",
    "\n",
    "# Forward\n",
    "input = random_tensor([25,14,14,1280],1)\n",
    "attention.qkv.weight.data = random_tensor([3840,1280],2)\n",
    "attention.qkv.bias.data = random_tensor([3840],3)\n",
    "attention.proj.weight.data = random_tensor([1280,1280],4)\n",
    "attention.proj.bias.data = random_tensor([1280],5)\n",
    "output = attention(input)\n",
    "items = [Item(\"input\", input, \"TensorFloat\"), Item(\"output\", output, \"TensorFloat\")]\n",
    "to_file(\"attention_forward\",items)\n",
    "del input\n",
    "del output\n",
    "del attention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d4e1c390",
   "metadata": {},
   "source": [
    "#### Image encoderViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83369d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024 16 3 1280 32 16 4.0 256 True <class 'torch.nn.modules.normalization.LayerNorm'> <class 'torch.nn.modules.activation.GELU'> True True True 14 [7, 15, 23, 31]\n",
      "torch.Size([1, 3, 1024, 1024])\n"
     ]
    }
   ],
   "source": [
    "from segment_anything.modeling.image_encoder import ImageEncoderViT\n",
    "\n",
    "image_encoder = ImageEncoderViT(1024,16,3,1280,32,16,4.0,256,True,nn.LayerNorm,nn.GELU,True,True,True,14,[7,15,23,31])\n",
    "items =[Item(\"img_size\", image_encoder.img_size,\"Int\")]\n",
    "to_file(\"image_encoder\",items)\n",
    "\n",
    "input = random_tensor([1,3,1024,1024],1)\n",
    "output = image_encoder(input)\n",
    "items = [Item(\"input\", input, \"TensorFloat\"), Item(\"output\", output, \"TensorFloat\")]\n",
    "to_file(\"image_encoder_forward\",items)\n",
    "\n",
    "del image_encoder\n",
    "del input\n",
    "del output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
