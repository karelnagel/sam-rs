{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c712610",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "from segment_anything.utils.onnx import SamOnnxModel\n",
    "\n",
    "import onnxruntime\n",
    "from onnxruntime.quantization import QuantType\n",
    "from onnxruntime.quantization.quantize import quantize_dynamic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f29441b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_mask(mask, ax):\n",
    "    color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "    \n",
    "def show_points(coords, labels, ax, marker_size=375):\n",
    "    pos_points = coords[labels==1]\n",
    "    neg_points = coords[labels==0]\n",
    "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)   \n",
    "    \n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76fc53f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"sam_vit_h_4b8939.pth\"\n",
    "model_type = \"vit_h\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11bfc8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 8 1\n",
      "256 8 2\n",
      "256 8 2\n",
      "256 8 1\n",
      "256 8 2\n",
      "256 8 2\n",
      "256 8 2\n"
     ]
    }
   ],
   "source": [
    "sam = sam_model_registry[model_type]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38a8add8",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_path = \"sam_onnx_example.onnx\"  # Set to use an already exported model, then skip to the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6be6eb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('../segment-anything/notebooks/images/truck.jpg')\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9689b1bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchFile",
     "evalue": "[ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from sam_onnx_example.onnx failed:Load model sam_onnx_example.onnx failed. File doesn't exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoSuchFile\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ort_session \u001b[39m=\u001b[39m onnxruntime\u001b[39m.\u001b[39;49mInferenceSession(onnx_model_path)\n",
      "File \u001b[0;32m~/Documents/projects/sam-rs/.venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:360\u001b[0m, in \u001b[0;36mInferenceSession.__init__\u001b[0;34m(self, path_or_bytes, sess_options, providers, provider_options, **kwargs)\u001b[0m\n\u001b[1;32m    357\u001b[0m disabled_optimizers \u001b[39m=\u001b[39m kwargs[\u001b[39m\"\u001b[39m\u001b[39mdisabled_optimizers\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mdisabled_optimizers\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m kwargs \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    359\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 360\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_inference_session(providers, provider_options, disabled_optimizers)\n\u001b[1;32m    361\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[1;32m    362\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_enable_fallback:\n",
      "File \u001b[0;32m~/Documents/projects/sam-rs/.venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:397\u001b[0m, in \u001b[0;36mInferenceSession._create_inference_session\u001b[0;34m(self, providers, provider_options, disabled_optimizers)\u001b[0m\n\u001b[1;32m    395\u001b[0m session_options \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sess_options \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sess_options \u001b[39melse\u001b[39;00m C\u001b[39m.\u001b[39mget_default_session_options()\n\u001b[1;32m    396\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model_path:\n\u001b[0;32m--> 397\u001b[0m     sess \u001b[39m=\u001b[39m C\u001b[39m.\u001b[39;49mInferenceSession(session_options, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_model_path, \u001b[39mTrue\u001b[39;49;00m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_config_from_model)\n\u001b[1;32m    398\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    399\u001b[0m     sess \u001b[39m=\u001b[39m C\u001b[39m.\u001b[39mInferenceSession(session_options, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model_bytes, \u001b[39mFalse\u001b[39;00m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_read_config_from_model)\n",
      "\u001b[0;31mNoSuchFile\u001b[0m: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from sam_onnx_example.onnx failed:Load model sam_onnx_example.onnx failed. File doesn't exist"
     ]
    }
   ],
   "source": [
    "ort_session = onnxruntime.InferenceSession(onnx_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e067b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sam.to()\n",
    "predictor = SamPredictor(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad3f0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.set_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6f0f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embedding = predictor.get_image_embedding().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e112f33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 256, 64, 64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6337b654",
   "metadata": {},
   "source": [
    "The ONNX model has a different input signature than `SamPredictor.predict`. The following inputs must all be supplied. Note the special cases for both point and mask inputs. All inputs are `np.float32`.\n",
    "* `image_embeddings`: The image embedding from `predictor.get_image_embedding()`. Has a batch index of length 1.\n",
    "* `point_coords`: Coordinates of sparse input prompts, corresponding to both point inputs and box inputs. Boxes are encoded using two points, one for the top-left corner and one for the bottom-right corner. *Coordinates must already be transformed to long-side 1024.* Has a batch index of length 1.\n",
    "* `point_labels`: Labels for the sparse input prompts. 0 is a negative input point, 1 is a positive input point, 2 is a top-left box corner, 3 is a bottom-right box corner, and -1 is a padding point. *If there is no box input, a single padding point with label -1 and coordinates (0.0, 0.0) should be concatenated.*\n",
    "* `mask_input`: A mask input to the model with shape 1x1x256x256. This must be supplied even if there is no mask input. In this case, it can just be zeros.\n",
    "* `has_mask_input`: An indicator for the mask input. 1 indicates a mask input, 0 indicates no mask input.\n",
    "* `orig_im_size`: The size of the input image in (H,W) format, before any transformation. \n",
    "\n",
    "Additionally, the ONNX model does not threshold the output mask logits. To obtain a binary mask, threshold at `sam.mask_threshold` (equal to 0.0)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5a9f55",
   "metadata": {},
   "source": [
    "### Example point input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0deef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_point = np.array([[500, 375]])\n",
    "input_label = np.array([1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7256394c",
   "metadata": {},
   "source": [
    "Add a batch index, concatenate a padding point, and transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f69903e",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_coord = np.concatenate([input_point, np.array([[0.0, 0.0]])], axis=0)[None, :, :]\n",
    "onnx_label = np.concatenate([input_label, np.array([-1])], axis=0)[None, :].astype(np.float32)\n",
    "\n",
    "onnx_coord = predictor.transform.apply_coords(onnx_coord, image.shape[:2]).astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b188dc53",
   "metadata": {},
   "source": [
    "Create an empty mask input and an indicator for no mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb52bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_mask_input = np.zeros((1, 1, 256, 256), dtype=np.float32)\n",
    "onnx_has_mask_input = np.zeros(1, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99c2cc5",
   "metadata": {},
   "source": [
    "Package the inputs to run in the onnx model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d7ea11",
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_inputs = {\n",
    "    \"image_embeddings\": image_embedding,\n",
    "    \"point_coords\": onnx_coord,\n",
    "    \"point_labels\": onnx_label,\n",
    "    \"mask_input\": onnx_mask_input,\n",
    "    \"has_mask_input\": onnx_has_mask_input,\n",
    "    \"orig_im_size\": np.array(image.shape[:2], dtype=np.float32)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6409c9",
   "metadata": {},
   "source": [
    "Predict a mask and threshold it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4cc082",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ort_session' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m masks, _, low_res_logits \u001b[39m=\u001b[39m ort_session\u001b[39m.\u001b[39mrun(\u001b[39mNone\u001b[39;00m, ort_inputs)\n\u001b[1;32m      2\u001b[0m masks \u001b[39m=\u001b[39m masks \u001b[39m>\u001b[39m predictor\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mmask_threshold\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ort_session' is not defined"
     ]
    }
   ],
   "source": [
    "masks, _, low_res_logits = ort_session.run(None, ort_inputs)\n",
    "masks = masks > predictor.model.mask_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d778a8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badb1175",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(image)\n",
    "show_mask(masks, plt.gca())\n",
    "show_points(input_point, input_label, plt.gca())\n",
    "plt.axis('off')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1d4d15",
   "metadata": {},
   "source": [
    "### Example mask input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b319da82",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_point = np.array([[500, 375], [1125, 625]])\n",
    "input_label = np.array([1, 1])\n",
    "\n",
    "# Use the mask output from the previous run. It is already in the correct form for input to the ONNX model.\n",
    "onnx_mask_input = low_res_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1823b37",
   "metadata": {},
   "source": [
    "Transform the points as in the previous example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8885130f",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_coord = np.concatenate([input_point, np.array([[0.0, 0.0]])], axis=0)[None, :, :]\n",
    "onnx_label = np.concatenate([input_label, np.array([-1])], axis=0)[None, :].astype(np.float32)\n",
    "\n",
    "onnx_coord = predictor.transform.apply_coords(onnx_coord, image.shape[:2]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e47b69",
   "metadata": {},
   "source": [
    "The `has_mask_input` indicator is now 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab4483a",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_has_mask_input = np.ones(1, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3781955",
   "metadata": {},
   "source": [
    "Package inputs, then predict and threshold the mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1ec096",
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_inputs = {\n",
    "    \"image_embeddings\": image_embedding,\n",
    "    \"point_coords\": onnx_coord,\n",
    "    \"point_labels\": onnx_label,\n",
    "    \"mask_input\": onnx_mask_input,\n",
    "    \"has_mask_input\": onnx_has_mask_input,\n",
    "    \"orig_im_size\": np.array(image.shape[:2], dtype=np.float32)\n",
    "}\n",
    "\n",
    "masks, _, _ = ort_session.run(None, ort_inputs)\n",
    "masks = masks > predictor.model.mask_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e36554b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(image)\n",
    "show_mask(masks, plt.gca())\n",
    "show_points(input_point, input_label, plt.gca())\n",
    "plt.axis('off')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef211d0",
   "metadata": {},
   "source": [
    "### Example box and point input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e58d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_box = np.array([425, 600, 700, 875])\n",
    "input_point = np.array([[575, 750]])\n",
    "input_label = np.array([0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e119dcb",
   "metadata": {},
   "source": [
    "Add a batch index, concatenate a box and point inputs, add the appropriate labels for the box corners, and transform. There is no padding point since the input includes a box input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbe4911",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_box_coords = input_box.reshape(2, 2)\n",
    "onnx_box_labels = np.array([2,3])\n",
    "\n",
    "onnx_coord = np.concatenate([input_point, onnx_box_coords], axis=0)[None, :, :]\n",
    "onnx_label = np.concatenate([input_label, onnx_box_labels], axis=0)[None, :].astype(np.float32)\n",
    "\n",
    "onnx_coord = predictor.transform.apply_coords(onnx_coord, image.shape[:2]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65edabd2",
   "metadata": {},
   "source": [
    "Package inputs, then predict and threshold the mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abfba56",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_mask_input = np.zeros((1, 1, 256, 256), dtype=np.float32)\n",
    "onnx_has_mask_input = np.zeros(1, dtype=np.float32)\n",
    "\n",
    "ort_inputs = {\n",
    "    \"image_embeddings\": image_embedding,\n",
    "    \"point_coords\": onnx_coord,\n",
    "    \"point_labels\": onnx_label,\n",
    "    \"mask_input\": onnx_mask_input,\n",
    "    \"has_mask_input\": onnx_has_mask_input,\n",
    "    \"orig_im_size\": np.array(image.shape[:2], dtype=np.float32)\n",
    "}\n",
    "\n",
    "masks, _, _ = ort_session.run(None, ort_inputs)\n",
    "masks = masks > predictor.model.mask_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8301bf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image)\n",
    "show_mask(masks[0], plt.gca())\n",
    "show_box(input_box, plt.gca())\n",
    "show_points(input_point, input_label, plt.gca())\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
